{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # OS e.g directory structure\n",
    "import numpy as np # linear algebra\n",
    "import scipy as sc  # scientific computing\n",
    "import pandas as pd # data processing, file I/O\n",
    "import seaborn as sns  # visualization\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 22983760\n",
      "drwxr-xr-x  5 macbookpro  staff          160 19 Mai 10:41 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  6 macbookpro  staff          192  6 Mai 14:27 \u001b[34m..\u001b[m\u001b[m\n",
      "drwxr-xr-x  3 macbookpro  staff           96  7 Mai 00:31 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--@ 1 macbookpro  staff        13157 19 Mai 10:41 TP_BIG_DATA.ipynb\n",
      "-rw-rw-r--@ 1 macbookpro  staff  11764863851  1 Dez  2019 library-collection-inventory.csv\n",
      "BibNum,Title,Author,ISBN,PublicationYear,Publisher,Subjects,ItemType,ItemCollection,FloatingItem,ItemLocation,ReportDate,ItemCount\n",
      "3011076,\"A tale of two friends / adapted by Ellie O'Ryan ; illustrated by Tom Caulfield, Frederick Gardner, Megan Petasky, and Allen Tam.\",\"O'Ryan, Ellie\",\"1481425730, 1481425749, 9781481425735, 9781481425742\",2014.,\"Simon Spotlight,\",\"Musicians Fiction, Bullfighters Fiction, Best friends Fiction, Friendship Fiction, Adventure and adventurers Fiction\",jcbk,ncrdr,Floating,qna,2017-09-01T00:00:00.000,1\n",
      "2248846,\"Naruto. Vol. 1, Uzumaki Naruto / story and art by Masashi Kishimoto ; [English adaptation by Jo Duffy].\",\"Kishimoto, Masashi, 1974-\",1569319006,\"2003, c1999.\",\"Viz,\",\"Ninja Japan Comic books strips etc, Comic books strips etc Japan Translations into English, Graphic novels\",acbk,nycomic,NA,lcy,2017-09-01T00:00:00.000,1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! ls -la\n",
    "! head -n 3 library-collection-inventory.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType, DoubleType,ArrayType\n",
    "\n",
    "fire_schema = StructType([StructField(\"BibNum\", IntegerType(),True),\n",
    "                             StructField(\"Title\", StringType(),True),\n",
    "                             StructField(\"Author\", StringType(),True),\n",
    "                             StructField(\"ISBN\", IntegerType(),True),\n",
    "                             StructField(\"PublicationYear\", IntegerType(),True),\n",
    "                             StructField(\"Publisher\", StringType(),True),\n",
    "                             StructField(\"Subjects\", StringType(), True),\n",
    "                             StructField(\"ItemType\", StringType(),True),\n",
    "                             StructField(\"ItemCollection\", StringType(),True),\n",
    "                             StructField(\"FloatingItem\", StringType(),True),\n",
    "                             StructField(\"ItemLocation\", StringType(),True),\n",
    "                             StructField(\"ReportDate\", TimestampType(),True),\n",
    "                             StructField(\"ItemCount\", IntegerType(),True)\n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = spark.read.csv('library-collection-inventory.csv',header=True, schema=fire_schema, sep=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BibNum: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Author: string (nullable = true)\n",
      " |-- ISBN: integer (nullable = true)\n",
      " |-- PublicationYear: integer (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- Subjects: string (nullable = true)\n",
      " |-- ItemType: string (nullable = true)\n",
      " |-- ItemCollection: string (nullable = true)\n",
      " |-- FloatingItem: string (nullable = true)\n",
      " |-- ItemLocation: string (nullable = true)\n",
      " |-- ReportDate: timestamp (nullable = true)\n",
      " |-- ItemCount: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35531308"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "cols = df.columns\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.dropna(subset=[\"Author\",\"Publisher\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Subjects|\n",
      "+--------------------+\n",
      "|Grandfathers Juve...|\n",
      "|Brigham Young Uni...|\n",
      "|Cooking Japanese,...|\n",
      "|Counting Fiction,...|\n",
      "|Parents Death Fic...|\n",
      "|Missing persons F...|\n",
      "|Paper Juvenile li...|\n",
      "|Mount Rainier Nat...|\n",
      "|Giraffe Juvenile ...|\n",
      "|Sharks Juvenile f...|\n",
      "|Undercover operat...|\n",
      "|                null|\n",
      "|Superheroes Comic...|\n",
      "|                null|\n",
      "|Probation Fiction...|\n",
      "|Unemployment Juve...|\n",
      "|Rand Paul 1914 19...|\n",
      "|                null|\n",
      "|Russo Japanese Wa...|\n",
      "|Quilting, Quiltin...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.select(\"Subjects\").filter(\"PublicationYear is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_row =10000\n",
    "\n",
    "df_clean = df_1.limit(max_row)\n",
    "\n",
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def transformToIndex(inputArr, data):\n",
    "    indexer=None \n",
    "    for col_in, col_out in inputArr.items():\n",
    "        \n",
    "        indexer = StringIndexer(inputCol=col_in, outputCol=col_out)\n",
    "        data = indexer.fit(data).transform(data)\n",
    "        \n",
    "        # conver√ßao para int \n",
    "        data = data.withColumn(col_out, data[col_out].cast(IntegerType()))\n",
    "    return data\n",
    "col_to_trasform = {'ItemType': 'ItemTypeIndex',\n",
    "                   'Author':'AuthorIndex',\n",
    "                   'Publisher':'PublisherIndex',\n",
    "                   'ItemCollection': 'ItemCollectionIndex'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = transformToIndex(col_to_trasform,df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformToIndex(col_to_trasform,df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BibNum: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Author: string (nullable = true)\n",
      " |-- ISBN: integer (nullable = true)\n",
      " |-- PublicationYear: integer (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- Subjects: string (nullable = true)\n",
      " |-- ItemType: string (nullable = true)\n",
      " |-- ItemCollection: string (nullable = true)\n",
      " |-- FloatingItem: string (nullable = true)\n",
      " |-- ItemLocation: string (nullable = true)\n",
      " |-- ReportDate: timestamp (nullable = true)\n",
      " |-- ItemCount: integer (nullable = true)\n",
      " |-- ItemTypeIndex: integer (nullable = true)\n",
      " |-- AuthorIndex: integer (nullable = true)\n",
      " |-- PublisherIndex: integer (nullable = true)\n",
      " |-- ItemCollectionIndex: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento da Variavel Preditora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversao de string para array\n",
    "df_clean = df_clean.withColumn('Subjects_split', split(col('Subjects'), \",\").cast(\"array<string>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BibNum: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Author: string (nullable = true)\n",
      " |-- ISBN: integer (nullable = true)\n",
      " |-- PublicationYear: integer (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- Subjects: string (nullable = true)\n",
      " |-- ItemType: string (nullable = true)\n",
      " |-- ItemCollection: string (nullable = true)\n",
      " |-- FloatingItem: string (nullable = true)\n",
      " |-- ItemLocation: string (nullable = true)\n",
      " |-- ReportDate: timestamp (nullable = true)\n",
      " |-- ItemCount: integer (nullable = true)\n",
      " |-- ItemTypeIndex: integer (nullable = true)\n",
      " |-- AuthorIndex: integer (nullable = true)\n",
      " |-- PublisherIndex: integer (nullable = true)\n",
      " |-- ItemCollectionIndex: integer (nullable = true)\n",
      " |-- Subjects_split: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+--------------+-----------+\n",
      "|      Subjects_split|PublisherIndex|AuthorIndex|\n",
      "+--------------------+--------------+-----------+\n",
      "|[Musicians Fictio...|            27|       5797|\n",
      "|[Ninja Japan Comi...|           493|        107|\n",
      "|[Duncan Jeremy Fi...|           193|        912|\n",
      "|[Hemingway Ernest...|          1565|       2020|\n",
      "|[Man woman relati...|           867|        243|\n",
      "|[Survival Juvenil...|           343|       5834|\n",
      "|[Cooking American...|             9|       3053|\n",
      "|[Forensic psychia...|             7|        861|\n",
      "|[Pirates Juvenile...|          3422|       2750|\n",
      "|[Alchemy Fiction,...|           556|       5136|\n",
      "|[Parrots,  Parrot...|          3631|       7413|\n",
      "|[Fragrant gardens...|           189|       4416|\n",
      "|[Graphic novels, ...|           900|         18|\n",
      "|[Bands Music Fict...|          1563|       1824|\n",
      "|[Lesbianism Ficti...|          1475|       4274|\n",
      "|[Aristocracy Soci...|           221|       4283|\n",
      "|[Astronomy Histor...|           252|       5397|\n",
      "|[Dogs Juvenile fi...|             8|        537|\n",
      "|[Heroes Juvenile ...|            20|       1706|\n",
      "|[Schools Fiction,...|            24|       6150|\n",
      "+--------------------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()\n",
    "\n",
    "df_clean.select('Subjects_split','PublisherIndex','AuthorIndex').filter(\"Subjects is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column Subjects_split must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<string>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-12f34caab331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Subjects_split\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SubjectsIndex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxCategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mindexerModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m print(\"Chose %d categorical features: %s\" %\n\u001b[1;32m      5\u001b[0m       (len(categoricalFeatures), \", \".join(str(k) for k in categoricalFeatures.keys())))\n",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column Subjects_split must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<string>."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop(\"PublisherIndex\")\\\n",
    ".printSchema()\n",
    "#df_clean.select('Author','Publisher').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.select('Author','Publisher','PublisherIndex').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['ItemCount','FloatingItem','BibNum','ReportDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_feactures = [c for c in cols if c not in col_to_drop]\n",
    "col_interesting = ['PublisherIndex','AuthorIndex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    "                .save(\"library-file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.1672540728248159], [0.1672540728248159, 1.0]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Correlation requires vectors so prior we convert to vector column\n",
    "\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=col_interesting, outputCol=vector_col)\n",
    "df_vector = assembler.transform(df_clean).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "\n",
    "matrix = Correlation.corr(df_vector, vector_col).collect()[0][0]\n",
    "corrmatrix = matrix.toArray().tolist()\n",
    "# corrmatrix\n",
    "corrmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScatterPlot(df, width, height):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.pcolor(df)\n",
    "    plt.yticks(np.arange(0.5, len(df.index), 1), df.index)\n",
    "    plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python DataFrame for visualization\n",
    "\n",
    "df_plot = pd.DataFrame(data=corrmatrix)\n",
    "# df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScatterPlot(df_plot, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe([\"AuthorIndex\",\"PublisherIndex\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
